OpenAI
https://openai.com/blog/spinning-up-in-deep-rl/
https://spinningup.openai.com/en/latest/

Keras
https://keras.io/models/model/

Weight initialization
https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/

Activation functions
https://en.wikipedia.org/wiki/Activation_function

Implementing backpropagation
https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/
https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536

Implementing fully connected networks
https://egghead.io/lessons/python-course-introduction-fully-connected-neural-networks-with-keras

3BLUE1BROWN SERIES  S3 â€¢ E1 - E4
https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi

ADAM optimizer
https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/

Kudos to Jason Brownlee, PhD for his series on machine learning
Great explanations, and sometimes even code!
https://machinelearningmastery.com/

Batch gradient descent:
It's been hard to find on internet how to handle batch gradient descent. Stochastic gradient descent is easy.
You feed forward once, backprop once, update the weights once. But in the case of batch, I was left wondering how to handle multiple feed forwards.
What do you do with the errors ? How do you backprop once?
The answer is that the batch gradient descent doesn save any backpropagation, it simply saves you weight updates.
During a mini batch, you backprop for each example, store it away and update the weights once after the bacth.
You have to save all the inputs and errors, and when you update the weights, average the gradient.

Momentum
https://www.youtube.com/watch?v=k8fTYJPd3_I - There's a nice slide at 6:28 which shows the equations